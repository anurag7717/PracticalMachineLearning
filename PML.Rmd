---
title: "PRACTICAL MACHINE LEARNING PROJECT"
output: html_document
---

## A Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.


## Loading the data and exploratory analysis

1. First, we shall load the packages required for the project.
```{r}
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(corrplot)
set.seed(77)
```


2. Next, we shall load the datasets and clean it appropriately.

```{r}

training <- read.csv("pml-training.csv")
testing  <- read.csv("pml-testing.csv")

inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE) # creating a partiton
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]
dim(TrainSet)
dim(TestSet)
```


3. As seen, there are 160 variables in both sets. But, there are many NA values here. So let us remove all these NA, NZv's and ID variables.

```{r}
NZV <- nearZeroVar(TrainSet) 
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]
dim(TrainSet)
dim(TestSet)

AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]
TestSet  <- TestSet[, AllNA==FALSE]
dim(TrainSet)
dim(TestSet)

TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet)
dim(TestSet)
```

After this cleaning, we can see that we have brought down the number of variables to 54!


4. Before starting the prediction model, let's just check the correlation between the variables.

```{r}
corMatrix <- cor(TrainSet[, -54])
corrplot(corMatrix, order = "FPC", method = "square", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

There are very few variables that are highly correlated, as seen by the plot.


## Buliding the Prediction Model

There are multiple ways to bulid the model, but as seen in the course, one of the best models to use is the Random Forest method. So, I shall be using that method.

```{r}
set.seed(77)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modFitRF <- train(classe ~ ., data=TrainSet, method="rf",
                          trControl=controlRF)
modFitRF$finalModel

# Prediction against the Test Set
predictRF <- predict(modFitRF, newdata=TestSet)
confMatRF <- confusionMatrix(table(predictRF, TestSet$classe))
confMatRF

# Plotting the matrix
plot(confMatRF$table, col = confMatRF$byClass, 
     main = paste("Random Forest - Accuracy =",
                  round(confMatRF$overall['Accuracy'], 4)))
```

## Results

As we can see, the Random Forest method is giving and accuracy of 99.81%, which is pretty high. So, we can use this to answer the quiz.




















































